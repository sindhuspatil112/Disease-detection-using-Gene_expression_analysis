{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee84464c-e614-4244-a6e3-7622cc71e340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Preprocessing breast dataset...\n",
      "Initial shape: (13299, 87)\n",
      "âœ… Saved cleaned dataset: ..\\data\\cleaned\\breast_cleaned.csv (shape: (13299, 87))\n",
      "\n",
      "ðŸ”¹ Preprocessing ovarian dataset...\n",
      "Initial shape: (22189, 64)\n",
      "âœ… Saved cleaned dataset: ..\\data\\cleaned\\ovarian_cleaned.csv (shape: (22189, 64))\n",
      "\n",
      "ðŸ”¹ Preprocessing lung dataset...\n",
      "Initial shape: (22189, 247)\n",
      "âœ… Saved cleaned dataset: ..\\data\\cleaned\\lung_cleaned.csv (shape: (22189, 247))\n",
      "\n",
      "ðŸŽ‰ All datasets preprocessed and saved in data/cleaned/\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# Preprocessing All Gene Expression Datasets\n",
    "# ================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 1. Define paths\n",
    "# -----------------------------------------------\n",
    "base_dir = Path(\"../data\")\n",
    "processed_dir = base_dir / \"processed\"\n",
    "cleaned_dir = base_dir / \"cleaned\"\n",
    "cleaned_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "datasets = {\n",
    "    \"breast\": processed_dir / \"breast_processed.csv\",\n",
    "    \"ovarian\": processed_dir / \"ovarian_processed.csv\",\n",
    "    \"lung\": processed_dir / \"lung_processed.csv\"\n",
    "}\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 2. Define helper function\n",
    "# -----------------------------------------------\n",
    "def preprocess_dataset(file_path, disease_name):\n",
    "    print(f\"\\nðŸ”¹ Preprocessing {disease_name} dataset...\")\n",
    "\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Initial shape: {df.shape}\")\n",
    "\n",
    "    # Drop duplicate genes (if any)\n",
    "    df = df.drop_duplicates(subset=[\"Gene\"], keep=\"first\")\n",
    "\n",
    "    # Drop rows with too many missing values\n",
    "    df = df.dropna(thresh=int(df.shape[1] * 0.8))\n",
    "\n",
    "    # Replace remaining NaNs with column means\n",
    "    df = df.fillna(df.mean(numeric_only=True))\n",
    "\n",
    "    # Normalize numeric columns (gene expression values)\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    scaler = MinMaxScaler()\n",
    "    df[numeric_df.columns] = scaler.fit_transform(numeric_df)\n",
    "\n",
    "    # Save cleaned dataset\n",
    "    out_path = cleaned_dir / f\"{disease_name}_cleaned.csv\"\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"âœ… Saved cleaned dataset: {out_path} (shape: {df.shape})\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 3. Run preprocessing for all datasets\n",
    "# -----------------------------------------------\n",
    "for disease, path in datasets.items():\n",
    "    preprocess_dataset(path, disease)\n",
    "\n",
    "print(\"\\nðŸŽ‰ All datasets preprocessed and saved in data/cleaned/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51922227-6e61-40f2-8cde-1ecd5f8c97d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Adding labels for breast dataset...\n",
      "ðŸ§¬ Found 86 samples in metadata.\n",
      "âœ… Labels extracted: 43 cancer / 43 normal.\n",
      "âœ… Saved labeled dataset: ..\\data\\labeled\\breast_labeled.csv\n",
      "\n",
      "ðŸ”¹ Adding labels for ovarian dataset...\n",
      "ðŸ§¬ Found 63 samples in metadata.\n",
      "âœ… Labels extracted: 53 cancer / 10 normal.\n",
      "âœ… Saved labeled dataset: ..\\data\\labeled\\ovarian_labeled.csv\n",
      "\n",
      "ðŸ”¹ Adding labels for lung dataset...\n",
      "ðŸ§¬ Found 246 samples in metadata.\n",
      "âœ… Labels extracted: 0 cancer / 246 normal.\n",
      "âœ… Saved labeled dataset: ..\\data\\labeled\\lung_labeled.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "data_dir = Path(\"../data\")\n",
    "mapped_dir = data_dir / \"mapped\"\n",
    "cleaned_dir = data_dir / \"cleaned\"\n",
    "labeled_dir = data_dir / \"labeled\"\n",
    "labeled_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "datasets = {\n",
    "    \"breast\": data_dir / \"GSE15852_series_matrix.txt\",\n",
    "    \"ovarian\": data_dir / \"GSE18520_series_matrix.txt\",\n",
    "    \"lung\": data_dir / \"GSE31210_series_matrix.txt\"\n",
    "}\n",
    "\n",
    "processed_dir = data_dir / \"preprocessed\"  # ðŸ‘ˆ change directory here\n",
    "\n",
    "processed_files = {\n",
    "    \"breast\": processed_dir / \"breast_cleaned.csv\",\n",
    "    \"ovarian\": processed_dir / \"ovarian_cleaned.csv\",\n",
    "    \"lung\": processed_dir / \"lung_cleaned.csv\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_labels(gse_file):\n",
    "    \"\"\"\n",
    "    Extracts binary labels (0=Normal, 1=Cancer) for each sample\n",
    "    from GEO meta info like \"Normal BC0043N\" \"Cancer BC0043T\".\n",
    "    \"\"\"\n",
    "    with open(gse_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Get all sample-related metadata lines\n",
    "    sample_lines = [l.strip() for l in lines if l.startswith(\"!Sample_title\") or \"Sample_title\" in l]\n",
    "    \n",
    "    if not sample_lines:\n",
    "        raise ValueError(\"âŒ No sample title lines found in metadata.\")\n",
    "    \n",
    "    # Merge and split on quotes to isolate each sample title\n",
    "    text = \" \".join(sample_lines)\n",
    "    samples = re.findall(r'\"([^\"]+)\"', text)\n",
    "    \n",
    "    if not samples:\n",
    "        raise ValueError(\"âŒ No samples found within quotes in metadata.\")\n",
    "\n",
    "    print(f\"ðŸ§¬ Found {len(samples)} samples in metadata.\")\n",
    "    \n",
    "    # Determine label (1 for cancer/tumor, 0 for normal/control)\n",
    "    labels = []\n",
    "    for s in samples:\n",
    "        s_lower = s.lower()\n",
    "        if \"cancer\" in s_lower or \"tumor\" in s_lower or \"malignant\" in s_lower:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    \n",
    "    print(f\"âœ… Labels extracted: {sum(labels)} cancer / {len(labels) - sum(labels)} normal.\")\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "for disease, gse_file in datasets.items():\n",
    "    print(f\"\\nðŸ”¹ Adding labels for {disease} dataset...\")\n",
    "    try:\n",
    "        labels = extract_labels(gse_file)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error extracting labels for {disease}: {e}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(processed_files[disease])\n",
    "\n",
    "    # Transpose (genes as columns â†’ samples as rows)\n",
    "    df_t = df.set_index(\"Gene\").T\n",
    "\n",
    "    # Check matching length\n",
    "    if len(labels) != df_t.shape[0]:\n",
    "        print(f\"âš ï¸ Label/sample mismatch: {len(labels)} labels vs {df_t.shape[0]} samples. Adjusting to min length.\")\n",
    "        min_len = min(len(labels), df_t.shape[0])\n",
    "        labels = labels[:min_len]\n",
    "        df_t = df_t.iloc[:min_len]\n",
    "\n",
    "    df_t[\"status\"] = labels\n",
    "    out_path = labeled_dir / f\"{disease}_labeled.csv\"\n",
    "    df_t.to_csv(out_path, index=False)\n",
    "    print(f\"âœ… Saved labeled dataset: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a3cc4d7-9a0b-454d-8388-bce15fb4ef01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¬ Found 246 tissue samples in metadata.\n",
      "âœ… Labels extracted: 226 cancer / 20 normal.\n",
      "âœ… Saved labeled lung dataset: ..\\data\\labeled\\lung_labeled.csv\n",
      "\n",
      "ðŸ“Š Dataset Summary:\n",
      "Samples: 246\n",
      "Genes: 22189\n",
      "status\n",
      "1    226\n",
      "0     20\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# ----------------------------\n",
    "# Define paths\n",
    "# ----------------------------\n",
    "data_dir = Path(\"../data\")\n",
    "gse_file = data_dir / \"GSE31210_series_matrix.txt\"\n",
    "processed_file = data_dir / \"preprocessed\" / \"lung_cleaned.csv\"\n",
    "labeled_dir = data_dir / \"labeled\"\n",
    "labeled_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_path = labeled_dir / \"lung_labeled.csv\"\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Extract labels specifically from \"tissue:\" entries\n",
    "# ----------------------------\n",
    "def extract_labels_lung(gse_file):\n",
    "    \"\"\"\n",
    "    Extracts binary labels (1=cancer/tumor/adenocarcinoma, 0=normal)\n",
    "    for the lung dataset (GSE31210).\n",
    "    Only considers 'tissue:' metadata lines.\n",
    "    \"\"\"\n",
    "    with open(gse_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # âœ… Filter only lines describing tissue info\n",
    "    tissue_lines = [\n",
    "        l.strip() for l in lines\n",
    "        if l.startswith(\"!Sample_characteristics_ch1\") and \"tissue:\" in l.lower()\n",
    "    ]\n",
    "\n",
    "    # Extract each \"tissue: ...\" entry\n",
    "    text = \" \".join(tissue_lines)\n",
    "    tissues = re.findall(r'\"tissue: ([^\"]+)\"', text)\n",
    "\n",
    "    if not tissues:\n",
    "        raise ValueError(\"âŒ No 'tissue:' entries found in GEO file.\")\n",
    "    print(f\"ðŸ§¬ Found {len(tissues)} tissue samples in metadata.\")\n",
    "\n",
    "    # Convert to binary labels\n",
    "    labels = []\n",
    "    for t in tissues:\n",
    "        t_lower = t.lower()\n",
    "        if any(word in t_lower for word in [\"tumor\", \"cancer\", \"adenocarcinoma\", \"carcinoma\", \"nsclc\"]):\n",
    "            labels.append(1)\n",
    "        elif \"normal\" in t_lower:\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            print(f\"âš ï¸ Unknown tissue type: {t}\")\n",
    "            labels.append(None)\n",
    "\n",
    "    print(f\"âœ… Labels extracted: {sum(l == 1 for l in labels)} cancer / {sum(l == 0 for l in labels)} normal.\")\n",
    "    return labels\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Apply labels to dataset\n",
    "# ----------------------------\n",
    "labels = extract_labels_lung(gse_file)\n",
    "df = pd.read_csv(processed_file)\n",
    "\n",
    "# Transpose (genes as columns â†’ samples as rows)\n",
    "df_t = df.set_index(\"Gene\").T\n",
    "\n",
    "# Align label count and sample count\n",
    "min_len = min(len(labels), df_t.shape[0])\n",
    "df_t = df_t.iloc[:min_len]\n",
    "labels = labels[:min_len]\n",
    "\n",
    "df_t[\"status\"] = labels\n",
    "\n",
    "# Save labeled dataset\n",
    "df_t.to_csv(out_path, index=False)\n",
    "print(f\"âœ… Saved labeled lung dataset: {out_path}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nðŸ“Š Dataset Summary:\")\n",
    "print(f\"Samples: {df_t.shape[0]}\")\n",
    "print(f\"Genes: {df_t.shape[1] - 1}\")\n",
    "print(df_t['status'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15a69f49-d9c6-4c14-851a-3ac5b5eaa65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "df_processed = pd.read_csv(\"../data/preprocessed/breast_cleaned.csv\")   # genes as rows, samples as columns\n",
    "df_labeled = pd.read_csv(\"../data/labeled/breast_labeled.csv\")         # samples as rows, with status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a80bf148-e49e-4140-b486-c72a13b67075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract status values from the labeled file\n",
    "status_values = df_labeled[\"status\"].values\n",
    "\n",
    "# Get sample names (columns in processed file)\n",
    "sample_names = df_labeled.columns[:-1]  # all columns except 'status'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d9d1ff4-caa7-41ea-b3f2-527510e1af3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "13299 columns passed, passed data had 1 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:939\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:986\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[1;34m(content, columns)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[1;32m--> 986\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    987\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    989\u001b[0m     )\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_mi_list:\n\u001b[0;32m    991\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 13299 columns passed, passed data had 1 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create status DataFrame with same column order as processed dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m status_row \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([status_values], columns\u001b[38;5;241m=\u001b[39msample_names)\n\u001b[0;32m      3\u001b[0m status_row\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Append the status row at the bottom\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:851\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m--> 851\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m nested_data_to_arrays(\n\u001b[0;32m    852\u001b[0m         \u001b[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[0;32m    853\u001b[0m         \u001b[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[0;32m    854\u001b[0m         data,\n\u001b[0;32m    855\u001b[0m         columns,\n\u001b[0;32m    856\u001b[0m         index,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    857\u001b[0m         dtype,\n\u001b[0;32m    858\u001b[0m     )\n\u001b[0;32m    859\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    860\u001b[0m         arrays,\n\u001b[0;32m    861\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    865\u001b[0m     )\n\u001b[0;32m    866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[1;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m to_arrays(data, columns, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:845\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    842\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m    843\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m--> 845\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m _finalize_columns_and_data(arr, columns, dtype)\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:942\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[1;32m--> 942\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[0;32m    945\u001b[0m     contents \u001b[38;5;241m=\u001b[39m convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: 13299 columns passed, passed data had 1 columns"
     ]
    }
   ],
   "source": [
    "# Create status DataFrame with same column order as processed dataset\n",
    "status_row = pd.DataFrame([status_values], columns=sample_names)\n",
    "status_row.index = [\"status\"]\n",
    "\n",
    "# Append the status row at the bottom\n",
    "df_with_status = pd.concat([df_processed, status_row])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6338bfb-fbe0-48fd-b05f-63c56ee794a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed shape: (13299, 87)\n",
      "Labeled shape: (1, 13300)\n"
     ]
    }
   ],
   "source": [
    "print(\"Processed shape:\", df_processed.shape)\n",
    "print(\"Labeled shape:\", df_labeled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d09dd03-c2e0-4090-8083-b7d4de9b6324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning complete.\n",
      "Before: 13299\n",
      "After: 13909\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "path = \"../models/breast_genes.json\"\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    genes = json.load(f)\n",
    "\n",
    "expanded = []\n",
    "\n",
    "for g in genes:\n",
    "    g = g.strip()\n",
    "    \n",
    "    # Split genes like \"ABC1///ABC2\"\n",
    "    if \"///\" in g:\n",
    "        expanded.extend([p.strip() for p in g.split(\"///\") if p.strip()])\n",
    "    else:\n",
    "        expanded.append(g)\n",
    "\n",
    "# Remove duplicates while preserving order\n",
    "clean = list(dict.fromkeys(expanded))\n",
    "\n",
    "with open(path, \"w\") as f:\n",
    "    json.dump(clean, f, indent=2)\n",
    "\n",
    "print(\"Cleaning complete.\")\n",
    "print(\"Before:\", len(genes))\n",
    "print(\"After:\", len(clean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b8ce089-caac-4323-bddf-e45abf91ba92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning complete.\n",
      "Before: 22189\n",
      "After: 22836\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "path = \"../models/lung_genes.json\"\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    genes = json.load(f)\n",
    "\n",
    "expanded = []\n",
    "\n",
    "for g in genes:\n",
    "    g = g.strip()\n",
    "    \n",
    "    # Split genes like \"ABC1///ABC2\"\n",
    "    if \"///\" in g:\n",
    "        expanded.extend([p.strip() for p in g.split(\"///\") if p.strip()])\n",
    "    else:\n",
    "        expanded.append(g)\n",
    "\n",
    "# Remove duplicates while preserving order\n",
    "clean = list(dict.fromkeys(expanded))\n",
    "\n",
    "with open(path, \"w\") as f:\n",
    "    json.dump(clean, f, indent=2)\n",
    "\n",
    "print(\"Cleaning complete.\")\n",
    "print(\"Before:\", len(genes))\n",
    "print(\"After:\", len(clean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "451951e7-1c59-4d5b-bb7f-39be749eee59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning complete.\n",
      "Before: 22189\n",
      "After: 22836\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "path = \"../models/ovarian_genes.json\"\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    genes = json.load(f)\n",
    "\n",
    "expanded = []\n",
    "\n",
    "for g in genes:\n",
    "    g = g.strip()\n",
    "    \n",
    "    # Split genes like \"ABC1///ABC2\"\n",
    "    if \"///\" in g:\n",
    "        expanded.extend([p.strip() for p in g.split(\"///\") if p.strip()])\n",
    "    else:\n",
    "        expanded.append(g)\n",
    "\n",
    "# Remove duplicates while preserving order\n",
    "clean = list(dict.fromkeys(expanded))\n",
    "\n",
    "with open(path, \"w\") as f:\n",
    "    json.dump(clean, f, indent=2)\n",
    "\n",
    "print(\"Cleaning complete.\")\n",
    "print(\"Before:\", len(genes))\n",
    "print(\"After:\", len(clean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cb3147-983d-4afb-a231-08a77de67643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
